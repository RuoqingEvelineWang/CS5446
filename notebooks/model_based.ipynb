{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8VBkQbVbgIYB",
        "outputId": "d9d2dd46-2a63-4aba-9ddb-f090cbb49bcd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "id": "bXnCbYsPgXZN",
        "outputId": "d16faee2-7b3d-4f1c-ff9b-e257fc337cac"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>MAP</th>\n",
              "      <th>diastolic_bp</th>\n",
              "      <th>systolic_bp</th>\n",
              "      <th>urine</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>p_f_ratio</th>\n",
              "      <th>lactic_acid</th>\n",
              "      <th>serum_creatinine</th>\n",
              "      <th>GCS_total</th>\n",
              "      <th>fluid_boluses</th>\n",
              "      <th>vasopressors</th>\n",
              "      <th>PatientID</th>\n",
              "      <th>Timepoints</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  MAP  diastolic_bp  systolic_bp  urine  ALT  AST  p_f_ratio  \\\n",
              "0           0    3             1            2      2    0    0          0   \n",
              "1           1    2             1            2      2    0    0          1   \n",
              "2           2    3             1            2      2    0    0          0   \n",
              "3           3    0             1            1      2    0    0          1   \n",
              "4           4    2             2            1      2    1    0          0   \n",
              "\n",
              "   lactic_acid  serum_creatinine  GCS_total  fluid_boluses  vasopressors  \\\n",
              "0            0                 0          3              0             0   \n",
              "1            0                 0          3              0             0   \n",
              "2            0                 0          3              0             0   \n",
              "3            0                 0          3              0             0   \n",
              "4            0                 0          3              0             0   \n",
              "\n",
              "   PatientID  Timepoints  \n",
              "0          0           0  \n",
              "1          0           1  \n",
              "2          0           2  \n",
              "3          0           3  \n",
              "4          0           4  "
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Replace with your file path\n",
        "# file_path = '/content/drive/My Drive/binned_df.csv'\n",
        "file_path = '../data/binned_df.csv'\n",
        "\n",
        "binned_df = pd.read_csv(file_path)\n",
        "\n",
        "# Print some info\n",
        "binned_df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "3AgbLplkg4dj"
      },
      "outputs": [],
      "source": [
        "# state and action columns\n",
        "binned_state_columns = ['MAP', 'diastolic_bp', 'systolic_bp', 'urine', 'ALT', 'AST', 'lactic_acid', 'serum_creatinine', 'p_f_ratio', 'GCS_total']\n",
        "action_columns = ['fluid_boluses', 'vasopressors']\n",
        "\n",
        "def create_transitions(df):\n",
        "    X = []\n",
        "    y = []\n",
        "    patients = df['PatientID'].unique()\n",
        "\n",
        "    for patient in patients:\n",
        "        patient_records = df[df['PatientID'] == patient].reset_index(drop=True)\n",
        "        for i in range(len(patient_records) - 1):\n",
        "            current_state = patient_records.iloc[i][binned_state_columns]\n",
        "            next_state = patient_records.iloc[i + 1][binned_state_columns]\n",
        "            action = patient_records.iloc[i][action_columns]\n",
        "            X.append(np.concatenate([current_state, action]))\n",
        "            y.append(next_state)\n",
        "    return pd.DataFrame(X, columns=binned_state_columns + action_columns), pd.DataFrame(y, columns=binned_state_columns)\n",
        "\n",
        "# create transitions (model-based --> mb)\n",
        "X_mb, y_mb = create_transitions(binned_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pxsSiN_hToz",
        "outputId": "f6d41d5a-8f9c-45c4-c231-d58d811afbec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for predicting MAP: 0.50\n",
            "Accuracy for predicting diastolic_bp: 0.69\n",
            "Accuracy for predicting systolic_bp: 0.68\n",
            "Accuracy for predicting urine: 0.71\n",
            "Accuracy for predicting ALT: 0.81\n",
            "Accuracy for predicting AST: 0.80\n",
            "Accuracy for predicting lactic_acid: 0.93\n",
            "Accuracy for predicting serum_creatinine: 0.85\n",
            "Accuracy for predicting p_f_ratio: 0.63\n",
            "Accuracy for predicting GCS_total: 0.89\n"
          ]
        }
      ],
      "source": [
        "#use K nearest neighbours to calculate next state prediction (function approximation for transitions)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "X_mb_train, X_mb_test, y_mb_train, y_mb_test = train_test_split(X_mb, y_mb, test_size=0.2, random_state=42)\n",
        "knn_models = {col: KNeighborsClassifier(n_neighbors=20) for col in binned_state_columns}\n",
        "\n",
        "for col in binned_state_columns:\n",
        "    knn_models[col].fit(X_mb_train, y_mb_train[col])\n",
        "\n",
        "#evaluate accuracy\n",
        "predictions = pd.DataFrame()\n",
        "for col in binned_state_columns:\n",
        "    predictions[col] = knn_models[col].predict(X_mb_test)\n",
        "    accuracy = accuracy_score(y_mb_test[col], predictions[col])\n",
        "    print(f\"Accuracy for predicting {col}: {accuracy:.2f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-0bvCQ5bhbUC"
      },
      "outputs": [],
      "source": [
        "# reward function for binned values\n",
        "def get_unbinned_MAP(binned_MAP):\n",
        "    \"\"\"\n",
        "    Convert binned MAP values to representative unbinned values\n",
        "    MAP bins: 0: <55, 1: 55-60, 2: 60-65, 3: >65\n",
        "    Using conservative (lower) values for each bin\n",
        "    \"\"\"\n",
        "    MAP_values = {\n",
        "        0: 50,  # For <55, use 50 as representative\n",
        "        1: 55,  # For 55-60, use 55\n",
        "        2: 60,  # For 60-65, use 60\n",
        "        3: 65   # For >65, use 65\n",
        "    }\n",
        "    return MAP_values[binned_MAP]\n",
        "\n",
        "def get_unbinned_urine(binned_urine):\n",
        "    \"\"\"\n",
        "    Convert binned urine values to representative unbinned values\n",
        "    Urine bins: 0: <30, 1: 30-50, 2: >50\n",
        "    Using conservative (lower) values for each bin\n",
        "    \"\"\"\n",
        "    urine_values = {\n",
        "        0: 25,  # For <30, use 25 as representative\n",
        "        1: 30,  # For 30-50, use 30\n",
        "        2: 50   # For >50, use 50\n",
        "    }\n",
        "    return urine_values[binned_urine]\n",
        "\n",
        "def calculate_reward(MAP, urine):\n",
        "    \"\"\"Calculate reward based on unbinned MAP and urine values\"\"\"\n",
        "    # First check the special case for urine and MAP\n",
        "    if urine > 30 and MAP > 55:\n",
        "        return 0\n",
        "\n",
        "    # Otherwise, calculate reward based on MAP piecewise function\n",
        "    if MAP > 65:\n",
        "        return 0\n",
        "    elif 60 < MAP <= 65:\n",
        "        return (-0.05 * (65 - MAP)) / 5\n",
        "    elif 55 < MAP <= 60:\n",
        "        return (-0.1 * (60 - MAP)) / 5 - 0.05\n",
        "    else:  # MAP <= 55\n",
        "        return (-0.85 * (55 - MAP)) / 15 - 0.15\n",
        "\n",
        "def calculate_reward_from_binned(binned_MAP, binned_urine):\n",
        "    \"\"\"Calculate reward using binned values by converting to unbinned first\"\"\"\n",
        "    unbinned_MAP = get_unbinned_MAP(binned_MAP)\n",
        "    unbinned_urine = get_unbinned_urine(binned_urine)\n",
        "    return calculate_reward(unbinned_MAP, unbinned_urine)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3gt_MFCIhxKT"
      },
      "outputs": [],
      "source": [
        "# get reward\n",
        "binned_df[\"reward\"] = binned_df.apply(lambda row: calculate_reward_from_binned(row[\"MAP\"], row[\"urine\"]), axis=1)\n",
        "\n",
        "# get action\n",
        "binned_df['action_number'] = 4 * binned_df['fluid_boluses'] + binned_df['vasopressors']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kXVfLWC1jbsj",
        "outputId": "25479ed5-a6a5-4a31-9d78-cdb1d8df424e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train Data: (150144, 17)\n",
            "Test Data: (37536, 17)\n"
          ]
        }
      ],
      "source": [
        "#train test split\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "train_ratio = 0.8\n",
        "num_patients = binned_df['PatientID'].nunique()\n",
        "num_train_samples = int(num_patients * train_ratio)\n",
        "\n",
        "train_ids = np.random.choice(num_patients, num_train_samples, replace=False)\n",
        "train_mb_df = binned_df[binned_df[\"PatientID\"].isin(train_ids)].reset_index(drop=True)\n",
        "test_mb_df = binned_df[~binned_df[\"PatientID\"].isin(train_ids)].reset_index(drop=True)\n",
        "\n",
        "print(f'Train Data: {train_mb_df.shape}')\n",
        "print(f'Test Data: {test_mb_df.shape}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "4_5E4SvUj-Zn"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "from tqdm import tqdm\n",
        "\n",
        "# get dictionary\n",
        "train_state_dict = {tuple(row): idx for idx, row in enumerate(train_mb_df[binned_state_columns].drop_duplicates().values)}\n",
        "\n",
        "# helper functions\n",
        "def get_state_index(row, state_columns=binned_state_columns):\n",
        "    return train_state_dict[tuple(row[state_columns])]\n",
        "\n",
        "def find_closest_index(input_tuple, train_state_dict=train_state_dict):\n",
        "    closest_index = None\n",
        "    min_distance = float(\"inf\")\n",
        "\n",
        "    # Convert input tuple to numeric array\n",
        "    input_array = np.array(input_tuple).astype(float)\n",
        "\n",
        "    for state_tuple, index in train_state_dict.items():\n",
        "        # Convert state tuple to numeric array\n",
        "        state_array = np.array(state_tuple).astype(float)\n",
        "\n",
        "        # Calculate Euclidean distance\n",
        "        distance = np.linalg.norm(state_array - input_array)\n",
        "\n",
        "        if distance < min_distance:\n",
        "            min_distance = distance\n",
        "            closest_index = index\n",
        "\n",
        "    return closest_index\n",
        "\n",
        "state_size = len(train_state_dict)\n",
        "action_size = 16\n",
        "\n",
        "# return state action pair\n",
        "def return_state_action_df(row, fluid, vaso):\n",
        "    state_action = {}\n",
        "    for col in binned_state_columns:\n",
        "        state_action[col] = row[col].item()\n",
        "\n",
        "    state_action[\"fluid_boluses\"] = fluid\n",
        "    state_action[\"vasopressors\"] = vaso\n",
        "    return pd.DataFrame([state_action])\n",
        "\n",
        "def clinician_cumulative_reward(df, gamma=0.99):\n",
        "    patients = df[\"PatientID\"].unique()\n",
        "\n",
        "    total_cumulative_reward = 0\n",
        "    rewards = []\n",
        "\n",
        "    for patient in patients:\n",
        "        patient_reward = 0\n",
        "        patient_records = df[df['PatientID']==patient].reset_index(drop=True)\n",
        "        for i in range(len(patient_records) - 1): # 0 to 47 (exclusive of 47)\n",
        "            reward = patient_records.iloc[i][\"reward\"].item()\n",
        "            # times gamma (for patient trajectory)\n",
        "            patient_reward += (gamma ** i) * reward\n",
        "\n",
        "        rewards.append(patient_reward)\n",
        "        total_cumulative_reward += patient_reward\n",
        "\n",
        "    return total_cumulative_reward, rewards"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "MGYuvJoWkBLX"
      },
      "outputs": [],
      "source": [
        "# reward function for binned values\n",
        "def get_unbinned_MAP(binned_MAP):\n",
        "    \"\"\"\n",
        "    Convert binned MAP values to representative unbinned values\n",
        "    MAP bins: 0: <55, 1: 55-60, 2: 60-65, 3: >65\n",
        "    Using conservative (lower) values for each bin\n",
        "    \"\"\"\n",
        "    MAP_values = {\n",
        "        0: 50,  # For <55, use 50 as representative\n",
        "        1: 55,  # For 55-60, use 55\n",
        "        2: 60,  # For 60-65, use 60\n",
        "        3: 65   # For >65, use 65\n",
        "    }\n",
        "    return MAP_values[binned_MAP]\n",
        "\n",
        "def get_unbinned_urine(binned_urine):\n",
        "    \"\"\"\n",
        "    Convert binned urine values to representative unbinned values\n",
        "    Urine bins: 0: <30, 1: 30-50, 2: >50\n",
        "    Using conservative (lower) values for each bin\n",
        "    \"\"\"\n",
        "    urine_values = {\n",
        "        0: 25,  # For <30, use 25 as representative\n",
        "        1: 30,  # For 30-50, use 30\n",
        "        2: 50   # For >50, use 50\n",
        "    }\n",
        "    return urine_values[binned_urine]\n",
        "\n",
        "def calculate_reward(MAP, urine):\n",
        "    \"\"\"Calculate reward based on unbinned MAP and urine values\"\"\"\n",
        "    # First check the special case for urine and MAP\n",
        "    if urine > 30 and MAP > 55:\n",
        "        return 0\n",
        "\n",
        "    # Otherwise, calculate reward based on MAP piecewise function\n",
        "    if MAP > 65:\n",
        "        return 0\n",
        "    elif 60 < MAP <= 65:\n",
        "        return (-0.05 * (65 - MAP)) / 5\n",
        "    elif 55 < MAP <= 60:\n",
        "        return (-0.1 * (60 - MAP)) / 5 - 0.05\n",
        "    else:  # MAP <= 55\n",
        "        return (-0.85 * (55 - MAP)) / 15 - 0.15\n",
        "\n",
        "def calculate_reward_from_binned(binned_MAP, binned_urine):\n",
        "    \"\"\"Calculate reward using binned values by converting to unbinned first\"\"\"\n",
        "    unbinned_MAP = get_unbinned_MAP(binned_MAP)\n",
        "    unbinned_urine = get_unbinned_urine(binned_urine)\n",
        "    return calculate_reward(unbinned_MAP, unbinned_urine)\n",
        "\n",
        "\n",
        "class DynaQAgent:\n",
        "    def __init__(self, state_size, action_size, knn_models, gamma=0.99, alpha=0.1, planning_steps=10):\n",
        "        self.state_size = state_size\n",
        "        self.action_size = action_size\n",
        "        self.gamma = gamma\n",
        "        self.alpha = alpha\n",
        "        self.planning_steps = planning_steps\n",
        "        self.q_table = np.full((state_size, action_size), np.nan, dtype='float')\n",
        "        self.knn_models = knn_models\n",
        "        self.observed_pairs = set()\n",
        "        self.state_row_cache = {}\n",
        "\n",
        "    def initialize_q_values(self, train_df):\n",
        "        \"\"\"Initialize Q-values and state row cache efficiently\"\"\"\n",
        "        min_reward = np.min(train_df[\"reward\"])\n",
        "\n",
        "        # Pre-compute state row cache more efficiently\n",
        "        state_groups = train_df.groupby(binned_state_columns).first()\n",
        "        for state_tuple, row in state_groups.iterrows():\n",
        "            state_idx = train_state_dict[state_tuple]\n",
        "            self.state_row_cache[state_idx] = pd.Series(state_tuple, index=binned_state_columns)\n",
        "\n",
        "        # Initialize Q-values efficiently\n",
        "        states = train_df.apply(lambda row: get_state_index(row), axis=1).values\n",
        "        actions = train_df['action_number'].astype(int).values\n",
        "\n",
        "        # Initialize all to NaN first\n",
        "        self.q_table.fill(np.nan)\n",
        "\n",
        "        # Set min_reward for observed pairs\n",
        "        for state, action in zip(states, actions):\n",
        "            self.q_table[state, action] = min_reward\n",
        "            self.observed_pairs.add((state, action))\n",
        "\n",
        "        print(f\"Initialized {len(self.observed_pairs)} state-action pairs\")\n",
        "        print(f\"Cached {len(self.state_row_cache)} unique states\")\n",
        "\n",
        "    def predict_next_state(self, state_row, action):\n",
        "        \"\"\"Use KNN models to predict next state\"\"\"\n",
        "        fluid = action // 4\n",
        "        vaso = action % 4\n",
        "\n",
        "        state_action_df = return_state_action_df(state_row, fluid, vaso)\n",
        "\n",
        "        predicted_state = {}\n",
        "        for col in binned_state_columns:\n",
        "            predicted_state[col] = self.knn_models[col].predict(state_action_df)[0]\n",
        "\n",
        "        return predicted_state\n",
        "\n",
        "    def calculate_reward(self, predicted_state):\n",
        "        \"\"\"Calculate reward based on predicted state\"\"\"\n",
        "        return calculate_reward_from_binned(\n",
        "            predicted_state['MAP'],\n",
        "            predicted_state['urine']\n",
        "        )\n",
        "\n",
        "    def update(self, state, action, reward, next_state):\n",
        "        \"\"\"Update Q-values\"\"\"\n",
        "        next_max_q = np.nanmax(self.q_table[next_state, :])\n",
        "        self.q_table[state, action] += self.alpha * (\n",
        "            reward + self.gamma * next_max_q - self.q_table[state, action]\n",
        "        )\n",
        "\n",
        "\n",
        "def precompute_transitions(train_df):\n",
        "    \"\"\"\n",
        "    Precompute all valid transitions from the training data.\n",
        "\n",
        "    Args:\n",
        "        train_df: DataFrame containing training data\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary of patient transitions\n",
        "        list: List of observed state-action pairs\n",
        "    \"\"\"\n",
        "    print(\"Precomputing transitions...\")\n",
        "\n",
        "    # Initialize containers\n",
        "    patient_transitions = {}\n",
        "    observed_pairs = set()\n",
        "\n",
        "    # Process each patient\n",
        "    for patient_id in tqdm(train_df['PatientID'].unique()):\n",
        "        patient_data = train_df[train_df['PatientID'] == patient_id].sort_values('Timepoints')\n",
        "        patient_transitions[patient_id] = []\n",
        "\n",
        "        for i in range(len(patient_data) - 1):\n",
        "            current_row = patient_data.iloc[i]\n",
        "            next_row = patient_data.iloc[i + 1]\n",
        "\n",
        "            if current_row['Timepoints'] < 47:\n",
        "                # Get state indices and action\n",
        "                current_state = get_state_index(current_row)\n",
        "                next_state = get_state_index(next_row)\n",
        "                action = int(current_row['action_number'])\n",
        "\n",
        "                # Store transition\n",
        "                patient_transitions[patient_id].append({\n",
        "                    'state': current_state,\n",
        "                    'action': action,\n",
        "                    'reward': current_row['reward'],\n",
        "                    'next_state': next_state\n",
        "                })\n",
        "\n",
        "                # Add to observed pairs\n",
        "                observed_pairs.add((current_state, action))\n",
        "\n",
        "    print(f\"Processed {len(patient_transitions)} patients\")\n",
        "    print(f\"Found {len(observed_pairs)} unique state-action pairs\")\n",
        "\n",
        "    return patient_transitions, list(observed_pairs)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "n92J0GEbkHyi"
      },
      "outputs": [],
      "source": [
        "def train_dynaQ_orig(agent, patient_transitions, observed_pairs, num_iterations=100):\n",
        "    \"\"\"\n",
        "    DynaQ training with planning steps after each transition.\n",
        "    \"\"\"\n",
        "    print(\"Starting DynaQ training...\")\n",
        "    diff_tracker = np.zeros(num_iterations)\n",
        "    unseen_states_count = 0\n",
        "\n",
        "    # Pre-compute and cache data\n",
        "    planning_samples = np.array(observed_pairs)\n",
        "    state_tuple_to_idx = {tuple(agent.state_row_cache[idx]): idx\n",
        "                         for idx in agent.state_row_cache}\n",
        "\n",
        "    for iter_num in range(num_iterations):\n",
        "        q_old = agent.q_table.copy()\n",
        "        total_planning_steps = 0\n",
        "        total_transitions = 0\n",
        "\n",
        "        # Process each patient's transitions\n",
        "        for transitions in tqdm(patient_transitions.values(),\n",
        "                              desc=f\"Iteration {iter_num+1}/{num_iterations}\",\n",
        "                              leave=False):\n",
        "            # Process each real transition\n",
        "            for transition in transitions:\n",
        "                # Real experience update\n",
        "                agent.update(\n",
        "                    transition['state'],\n",
        "                    transition['action'],\n",
        "                    transition['reward'],\n",
        "                    transition['next_state']\n",
        "                )\n",
        "                total_transitions += 1\n",
        "\n",
        "                # Do planning steps after each transition\n",
        "                planning_indices = np.random.randint(\n",
        "                    0,\n",
        "                    len(planning_samples),\n",
        "                    size=agent.planning_steps\n",
        "                )\n",
        "                sampled_pairs = planning_samples[planning_indices]\n",
        "\n",
        "                # Batch planning steps\n",
        "                for sim_state, sim_action in sampled_pairs:\n",
        "                    sim_state_row = agent.state_row_cache[sim_state]\n",
        "\n",
        "                    # Model-based predictions\n",
        "                    predicted_next_state_dict = agent.predict_next_state(\n",
        "                        sim_state_row,\n",
        "                        sim_action\n",
        "                    )\n",
        "\n",
        "                    predicted_state_tuple = tuple(\n",
        "                        predicted_next_state_dict[col]\n",
        "                        for col in binned_state_columns\n",
        "                    )\n",
        "\n",
        "                    # Fast lookup with fallback\n",
        "                    predicted_next_state = state_tuple_to_idx.get(\n",
        "                        predicted_state_tuple,\n",
        "                        find_closest_index(predicted_state_tuple)\n",
        "                    )\n",
        "                    if predicted_next_state not in state_tuple_to_idx.values():\n",
        "                        unseen_states_count += 1\n",
        "\n",
        "                    # Update Q-values\n",
        "                    sim_reward = agent.calculate_reward(predicted_next_state_dict)\n",
        "                    agent.update(\n",
        "                        sim_state,\n",
        "                        sim_action,\n",
        "                        sim_reward,\n",
        "                        predicted_next_state\n",
        "                    )\n",
        "                    total_planning_steps += 1\n",
        "\n",
        "        # Calculate difference\n",
        "        valid_mask = ~np.isnan(agent.q_table)\n",
        "        diff = np.mean(np.abs(agent.q_table[valid_mask] - q_old[valid_mask]))\n",
        "        diff_tracker[iter_num] = diff\n",
        "\n",
        "        # Print info every 5 iterations\n",
        "        #if (iter_num + 1) % 5 == 0:\n",
        "        print(f\"\\n[Step {iter_num+1}/{num_iterations}]\")\n",
        "        print(f\"  Q-value difference: {diff:.6f}\")\n",
        "        print(f\"  Planning steps: {total_planning_steps}\")\n",
        "        print(f\"  Real transitions: {total_transitions}\")\n",
        "        print(f\"  Unseen states: {unseen_states_count}\")\n",
        "\n",
        "    return agent, diff_tracker\n",
        "\n",
        "def train_dynaQ(agent, patient_transitions, observed_pairs, num_iterations=100):\n",
        "    \"\"\"\n",
        "    DynaQ training with planning steps after each patient's transitions.\n",
        "    \"\"\"\n",
        "    print(\"Starting DynaQ training...\")\n",
        "    diff_tracker = np.zeros(num_iterations)\n",
        "    unseen_states_count = 0\n",
        "\n",
        "    # Pre-compute and cache data\n",
        "    planning_samples = np.array(observed_pairs)\n",
        "    state_tuple_to_idx = {tuple(agent.state_row_cache[idx]): idx\n",
        "                         for idx in agent.state_row_cache}\n",
        "\n",
        "    for iter_num in range(num_iterations):\n",
        "        q_old = agent.q_table.copy()\n",
        "        total_planning_steps = 0\n",
        "\n",
        "        # Process each patient's transitions\n",
        "        for transitions in tqdm(patient_transitions.values(),\n",
        "                              desc=f\"Iteration {iter_num+1}/{num_iterations}\",\n",
        "                              leave=False):\n",
        "            # First process all real transitions for this patient\n",
        "            for transition in transitions:\n",
        "                agent.update(\n",
        "                    transition['state'],\n",
        "                    transition['action'],\n",
        "                    transition['reward'],\n",
        "                    transition['next_state']\n",
        "                )\n",
        "\n",
        "            # Then do planning steps after patient's transitions\n",
        "            planning_indices = np.random.randint(\n",
        "                0,\n",
        "                len(planning_samples),\n",
        "                size=agent.planning_steps\n",
        "            )\n",
        "            sampled_pairs = planning_samples[planning_indices]\n",
        "\n",
        "            # Batch planning steps\n",
        "            for sim_state, sim_action in sampled_pairs:\n",
        "                sim_state_row = agent.state_row_cache[sim_state]\n",
        "\n",
        "                # Model-based predictions\n",
        "                predicted_next_state_dict = agent.predict_next_state(\n",
        "                    sim_state_row,\n",
        "                    sim_action\n",
        "                )\n",
        "\n",
        "                predicted_state_tuple = tuple(\n",
        "                    predicted_next_state_dict[col]\n",
        "                    for col in binned_state_columns\n",
        "                )\n",
        "\n",
        "                # Fast lookup with fallback\n",
        "                predicted_next_state = state_tuple_to_idx.get(\n",
        "                    predicted_state_tuple,\n",
        "                    find_closest_index(predicted_state_tuple)\n",
        "                )\n",
        "                if predicted_next_state not in state_tuple_to_idx.values():\n",
        "                    unseen_states_count += 1\n",
        "\n",
        "                # Update Q-values\n",
        "                sim_reward = agent.calculate_reward(predicted_next_state_dict)\n",
        "                agent.update(\n",
        "                    sim_state,\n",
        "                    sim_action,\n",
        "                    sim_reward,\n",
        "                    predicted_next_state\n",
        "                )\n",
        "                total_planning_steps += 1\n",
        "\n",
        "        # Calculate difference\n",
        "        valid_mask = ~np.isnan(agent.q_table)\n",
        "        diff = np.mean(np.abs(agent.q_table[valid_mask] - q_old[valid_mask]))\n",
        "        diff_tracker[iter_num] = diff\n",
        "\n",
        "        # Print info every 5 iterations\n",
        "        #if (iter_num + 1) % 5 == 0:\n",
        "        print(f\"\\n[Step {iter_num+1}/{num_iterations}]\")\n",
        "        print(f\"  Q-value difference: {diff:.6f}\")\n",
        "        print(f\"  Planning steps: {total_planning_steps}\")\n",
        "        print(f\"  Unseen states: {unseen_states_count}\")\n",
        "\n",
        "    return agent, diff_tracker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6O-kVZHEkIfa",
        "outputId": "9a501c9d-e51c-4bb3-e3df-bb966cd84b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precomputing transitions...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 3128/3128 [02:26<00:00, 21.32it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed 3128 patients\n",
            "Found 18175 unique state-action pairs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Precompute transitions\n",
        "patient_transitions, observed_pairs = precompute_transitions(train_mb_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0cyxnnGBkL3P",
        "outputId": "b97f33e7-40ea-4323-be41-675f531c06a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initialized 18282 state-action pairs\n",
            "Cached 10912 unique states\n"
          ]
        }
      ],
      "source": [
        "# Initialize agent\n",
        "dynaq_agent = DynaQAgent(state_size=state_size, action_size=action_size,\n",
        "                   knn_models=knn_models, gamma=0.99, alpha=0.1, planning_steps=5)\n",
        "dynaq_agent.initialize_q_values(train_mb_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "### CELINE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def train_dynaQ_orig(agent, patient_transitions, observed_pairs, num_iterations=100):\n",
        "    \"\"\"\n",
        "    DynaQ training with planning steps after each transition.\n",
        "    \"\"\"\n",
        "    print(\"Starting DynaQ training...\")\n",
        "    diff_tracker = np.zeros(num_iterations)\n",
        "    unseen_states_count = 0\n",
        "\n",
        "    # Pre-compute and cache data\n",
        "    planning_samples = np.array(observed_pairs)\n",
        "    state_tuple_to_idx = {tuple(agent.state_row_cache[idx]): idx\n",
        "                         for idx in agent.state_row_cache}\n",
        "\n",
        "    for iter_num in range(num_iterations):\n",
        "        q_old = agent.q_table.copy()\n",
        "        total_planning_steps = 0\n",
        "        total_transitions = 0\n",
        "\n",
        "        # Collect updates for batch processing\n",
        "        updates = []\n",
        "        \n",
        "        # Process each patient's transitions\n",
        "        for transitions in tqdm(patient_transitions.values(),\n",
        "                              desc=f\"Iteration {iter_num+1}/{num_iterations}\",\n",
        "                              leave=False):\n",
        "            for transition in transitions:\n",
        "                updates.append((transition['state'], transition['action'], transition['reward'], transition['next_state']))\n",
        "                total_transitions += 1\n",
        "\n",
        "        # Process all updates in a batch\n",
        "        for state, action, reward, next_state in updates:\n",
        "            agent.update(state, action, reward, next_state)\n",
        "\n",
        "        # Do planning steps after all real transitions\n",
        "        planning_indices = np.random.randint(\n",
        "            0,\n",
        "            len(planning_samples),\n",
        "            size=agent.planning_steps\n",
        "        )\n",
        "        sampled_pairs = planning_samples[planning_indices]\n",
        "\n",
        "        # Parallelize planning steps\n",
        "        def process_planning_step(sim_state, sim_action):\n",
        "            sim_state_row = agent.state_row_cache[sim_state]\n",
        "            predicted_next_state_dict = agent.predict_next_state(sim_state_row, sim_action)\n",
        "            predicted_state_tuple = tuple(predicted_next_state_dict[col] for col in binned_state_columns)\n",
        "            predicted_next_state = state_tuple_to_idx.get(predicted_state_tuple, find_closest_index(predicted_state_tuple))\n",
        "            return sim_state, sim_action, predicted_next_state, predicted_next_state_dict\n",
        "\n",
        "        results = Parallel(n_jobs=-1)(delayed(process_planning_step)(sim_state, sim_action) for sim_state, sim_action in sampled_pairs)\n",
        "\n",
        "        for sim_state, sim_action, predicted_next_state, predicted_next_state_dict in results:\n",
        "            if predicted_next_state not in state_tuple_to_idx.values():\n",
        "                unseen_states_count += 1\n",
        "            sim_reward = agent.calculate_reward(predicted_next_state_dict)\n",
        "            agent.update(sim_state, sim_action, sim_reward, predicted_next_state)\n",
        "            total_planning_steps += 1\n",
        "\n",
        "        # Calculate difference\n",
        "        valid_mask = ~np.isnan(agent.q_table)\n",
        "        diff = np.mean(np.abs(agent.q_table[valid_mask] - q_old[valid_mask]))\n",
        "        diff_tracker[iter_num] = diff\n",
        "\n",
        "        print(f\"\\n[Step {iter_num+1}/{num_iterations}]\")\n",
        "        print(f\"  Q-value difference: {diff:.6f}\")\n",
        "        print(f\"  Planning steps: {total_planning_steps}\")\n",
        "        print(f\"  Real transitions: {total_transitions}\")\n",
        "        print(f\"  Unseen states: {unseen_states_count}\")\n",
        "\n",
        "    # Plotting the convergence\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(diff_tracker[:iter_num + 1], marker='o')\n",
        "    plt.title('Convergence of DynaQ Agent')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Mean Absolute Q-value Difference')\n",
        "    plt.axhline(y=0, color='r', linestyle='--', label='Convergence Threshold (0)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return agent, diff_tracker\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "### CELINE\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def train_dynaQ_batch(agent, patient_transitions, observed_pairs, num_iterations=100, batch_size=10):\n",
        "    \"\"\"\n",
        "    DynaQ training with planning steps after a batch of transitions.\n",
        "    \"\"\"\n",
        "    print(\"Starting DynaQ training...\")\n",
        "    diff_tracker = np.zeros(num_iterations)\n",
        "    unseen_states_count = 0\n",
        "\n",
        "    # Pre-compute and cache data\n",
        "    planning_samples = np.array(observed_pairs)\n",
        "    state_tuple_to_idx = {tuple(agent.state_row_cache[idx]): idx\n",
        "                         for idx in agent.state_row_cache}\n",
        "\n",
        "    for iter_num in range(num_iterations):\n",
        "        q_old = agent.q_table.copy()\n",
        "        total_planning_steps = 0\n",
        "        total_transitions = 0\n",
        "\n",
        "        # Collect updates for batch processing\n",
        "        updates = []\n",
        "        \n",
        "        # Process each patient's transitions\n",
        "        for transitions in tqdm(patient_transitions.values(),\n",
        "                              desc=f\"Iteration {iter_num+1}/{num_iterations}\",\n",
        "                              leave=False):\n",
        "            for transition in transitions:\n",
        "                updates.append((transition['state'], transition['action'], transition['reward'], transition['next_state']))\n",
        "                total_transitions += 1\n",
        "\n",
        "                # If the batch size is reached, process the batch\n",
        "                if len(updates) == batch_size:\n",
        "                    # Process all updates in the batch\n",
        "                    for state, action, reward, next_state in updates:\n",
        "                        agent.update(state, action, reward, next_state)\n",
        "\n",
        "                    # Perform planning steps after the batch\n",
        "                    planning_indices = np.random.randint(\n",
        "                        0,\n",
        "                        len(planning_samples),\n",
        "                        size=agent.planning_steps\n",
        "                    )\n",
        "                    sampled_pairs = planning_samples[planning_indices]\n",
        "\n",
        "                    # Parallelize planning steps\n",
        "                    results = Parallel(n_jobs=-1)(delayed(process_planning_step)(sim_state, sim_action, agent, state_tuple_to_idx) for sim_state, sim_action in sampled_pairs)\n",
        "\n",
        "                    for sim_state, sim_action, predicted_next_state, predicted_next_state_dict in results:\n",
        "                        if predicted_next_state not in state_tuple_to_idx.values():\n",
        "                            unseen_states_count += 1\n",
        "                        sim_reward = agent.calculate_reward(predicted_next_state_dict)\n",
        "                        agent.update(sim_state, sim_action, sim_reward, predicted_next_state)\n",
        "                        total_planning_steps += 1\n",
        "\n",
        "                    # Clear the updates after processing\n",
        "                    updates = []\n",
        "\n",
        "        # Process any remaining transitions in the last batch\n",
        "        if updates:\n",
        "            for state, action, reward, next_state in updates:\n",
        "                agent.update(state, action, reward, next_state)\n",
        "\n",
        "            # Perform planning steps after the last batch\n",
        "            planning_indices = np.random.randint(\n",
        "                0,\n",
        "                len(planning_samples),\n",
        "                size=agent.planning_steps\n",
        "            )\n",
        "            sampled_pairs = planning_samples[planning_indices]\n",
        "\n",
        "            # Parallelize planning steps\n",
        "            results = Parallel(n_jobs=-1)(delayed(process_planning_step)(sim_state, sim_action, agent, state_tuple_to_idx) for sim_state, sim_action in sampled_pairs)\n",
        "\n",
        "            for sim_state, sim_action, predicted_next_state, predicted_next_state_dict in results:\n",
        "                if predicted_next_state not in state_tuple_to_idx.values():\n",
        "                    unseen_states_count += 1\n",
        "                sim_reward = agent.calculate_reward(predicted_next_state_dict)\n",
        "                agent.update(sim_state, sim_action, sim_reward, predicted_next_state)\n",
        "                total_planning_steps += 1\n",
        "\n",
        "        # Calculate difference\n",
        "        valid_mask = ~np.isnan(agent.q_table)\n",
        "        diff = np.mean(np.abs(agent.q_table[valid_mask] - q_old[valid_mask]))\n",
        "        diff_tracker[iter_num] = diff\n",
        "\n",
        "        print(f\"\\n[Step {iter_num+1}/{num_iterations}]\")\n",
        "        print(f\"  Q-value difference: {diff:.6f}\")\n",
        "        print(f\"  Planning steps: {total_planning_steps}\")\n",
        "        print(f\"  Real transitions: {total_transitions}\")\n",
        "        print(f\"  Unseen states: {unseen_states_count}\")\n",
        "\n",
        "    # Plotting the convergence\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(diff_tracker[:iter_num + 1], marker='o')\n",
        "    plt.title('Convergence of DynaQ Agent')\n",
        "    plt.xlabel('Iteration')\n",
        "    plt.ylabel('Mean Absolute Q-value Difference')\n",
        "    plt.axhline(y=0, color='r', linestyle='--', label='Convergence Threshold (0)')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    return agent, diff_tracker\n",
        "\n",
        "def process_planning_step(sim_state, sim_action, agent, state_tuple_to_idx):\n",
        "    sim_state_row = agent.state_row_cache[sim_state]\n",
        "    predicted_next_state_dict = agent.predict_next_state(sim_state_row, sim_action)\n",
        "    predicted_state_tuple = tuple(predicted_next_state_dict[col] for col in binned_state_columns)\n",
        "    predicted_next_state = state_tuple_to_idx.get(predicted_state_tuple, find_closest_index(predicted_state_tuple))\n",
        "    return sim_state, sim_action, predicted_next_state, predicted_next_state_dict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "gprTglPqkcfh",
        "outputId": "e21f4e28-7844-4f99-abbc-41d8bf1fd460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting DynaQ training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration 1/100:   0%|          | 0/3128 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Iteration 1/100:  16%|█▋        | 511/3128 [7:30:55<39:14:43, 53.99s/it]"
          ]
        }
      ],
      "source": [
        "# Train agent\n",
        "dynaq_agent, diff_tracker = train_dynaQ_batch(dynaq_agent, patient_transitions, observed_pairs, num_iterations=100, batch_size=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "MjMOvRIak6ly"
      },
      "outputs": [],
      "source": [
        "# save agent\n",
        "import pickle\n",
        "save_path = '../models/DynaQ/dynaq_agent_transition.pkl'\n",
        "\n",
        "with open(save_path, 'wb') as file:\n",
        "    pickle.dump(dynaq_agent, file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
