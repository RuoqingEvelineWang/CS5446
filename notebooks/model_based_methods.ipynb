{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  MAP  diastolic_bp  systolic_bp  urine  ALT  AST  PO2  \\\n",
      "0           0    0             1            2      2    0    0    1   \n",
      "1           1    0             1            2      2    0    0    2   \n",
      "2           2    0             1            2      2    0    0    1   \n",
      "3           3    0             1            1      2    0    0    2   \n",
      "4           4    0             2            1      2    1    0    0   \n",
      "\n",
      "   lactic_acid  serum_creatinine  FiO2  GCS_total  fluid_boluses  \\\n",
      "0            0                 0     2          2              0   \n",
      "1            0                 0     2          2              0   \n",
      "2            0                 0     2          2              0   \n",
      "3            0                 0     2          2              0   \n",
      "4            0                 0     2          2              0   \n",
      "\n",
      "   vasopressors  SOFA_score  PatientID  Timepoints  \n",
      "0             0           4          0           0  \n",
      "1             0           3          0           1  \n",
      "2             0           4          0           2  \n",
      "3             0           3          0           3  \n",
      "4             0           4          0           4  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('binned_df.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_columns = ['MAP', 'diastolic_bp', 'systolic_bp', 'urine', 'ALT', 'AST', 'PO2', 'lactic_acid', 'serum_creatinine', 'FiO2', 'GCS_total']\n",
    "action_columns = ['fluid_boluses', 'vasopressors']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reward function: SOFA score measures patient health (higher score/increase in score should be penalised)\n",
    "#referenced from: Deep Reinforcement Learning for Sepsis Treatment\n",
    "def calculate_reward(cur_score, next_score):\n",
    "    reward = 0\n",
    "    c0 = -0.025\n",
    "    c1 = -0.125\n",
    "    if cur_score == next_score and next_score > 0:\n",
    "        reward += c0\n",
    "    return reward + c1 * (next_score - cur_score)\n",
    "\n",
    "#calculate SOFA score for each state\n",
    "def calculate_sofa_score(MAP, urine, ALT, AST, PO2, lactic_acid, serum_creatinine, FiO2, GCS_total):\n",
    "    cardiovascular_score = 0\n",
    "    if MAP < 70:\n",
    "        cardiovascular_score = 1\n",
    "    \n",
    "    respitory_score = 0\n",
    "    if FiO2 > 0:\n",
    "        p_f_ratio = PO2 / FiO2\n",
    "        \n",
    "        if p_f_ratio < 100:\n",
    "            respitory_score = 4\n",
    "        elif p_f_ratio < 200:\n",
    "            respitory_score = 3\n",
    "        elif p_f_ratio < 300:\n",
    "            respitory_score = 2\n",
    "        else:\n",
    "            respitory_score = 1\n",
    "    \n",
    "    renal_score = 0\n",
    "    if serum_creatinine >= 1.2 and serum_creatinine < 2:\n",
    "        renal_score = 1\n",
    "    elif serum_creatinine >= 2 and serum_creatinine < 3.5:\n",
    "        renal_score = 2\n",
    "    elif serum_creatinine >= 3.5 and serum_creatinine < 5:\n",
    "        renal_score = 3\n",
    "    elif serum_creatinine >= 5.0:\n",
    "        renal_score = 4\n",
    "    #divide daily urine output standards by 6 as the data is 4 hour interval\n",
    "    if urine < 500.0 / 6:\n",
    "        renal_score = max(renal_score, 3)\n",
    "    if urine < 200.0 / 6:\n",
    "        renal_score = max(renal_score, 4)\n",
    "\n",
    "    #since bilirubin is not available, use ALT and AST to calculate liver score\n",
    "    liver_score = 0\n",
    "    if (ALT > 40 and AST > 40):\n",
    "        liver_score = 1\n",
    "    if (ALT > 120 or AST > 120):\n",
    "        liver_score = 2\n",
    "    if (ALT > 300 or AST > 300):\n",
    "        liver_score = 3\n",
    "    if (ALT > 1000 or AST > 1000):\n",
    "        liver_score = 4\n",
    "\n",
    "    neuro_score = 0\n",
    "    if GCS_total < 6:\n",
    "        neuro_score = 4\n",
    "    elif GCS_total < 10:\n",
    "        neuro_score = 3\n",
    "    elif GCS_total < 13:\n",
    "        neuro_score = 2\n",
    "    elif GCS_total < 15:\n",
    "        neuro_score = 1\n",
    "    \n",
    "    lactic_acid_score = 0\n",
    "    if lactic_acid > 2:\n",
    "        lactic_acid_score = 2\n",
    "    \n",
    "    return cardiovascular_score + respitory_score + renal_score + liver_score + neuro_score + lactic_acid_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for mdptoolbox Value Iteration, the reward function is (S, A), it doesn't use next state\n",
    "#this method only works with R(S, A) reward function\n",
    "import mdptoolbox\n",
    "import numpy as np\n",
    "\n",
    "states = df[['PatientID', 'Timepoints']].values\n",
    "num_states = len(states)\n",
    "num_actions = 25\n",
    "P = np.zeros((num_actions, num_states, num_states))\n",
    "R = np.zeros((num_states, num_actions))\n",
    "\n",
    "for i in range(len(df) - 1):\n",
    "    if df.iloc[i]['PatientID'] == df.iloc[i + 1]['PatientID'] and df.iloc[i]['Timepoints'] + 1 == df.iloc[i + 1]['Timepoints']:\n",
    "        current_state = i\n",
    "        next_state = i + 1\n",
    "        action = df.iloc[i]['ActionID']\n",
    "\n",
    "        reward = calculate_reward(df.iloc[i]['SOFA_score'], df.iloc[i + 1]['SOFA_score'])\n",
    "\n",
    "        P[action, current_state, next_state] += 1\n",
    "\n",
    "        R[current_state, action] = reward\n",
    "\n",
    "for action in range(num_actions):\n",
    "    for state in range(num_states):\n",
    "        if P[action, state].sum() > 0:\n",
    "            P[action, state] /= P[action, state].sum()\n",
    "\n",
    "vi = mdptoolbox.mdp.ValueIteration(P, R, 0.9)\n",
    "vi.run()\n",
    "\n",
    "print(\"Optimal Policy:\", vi.policy)\n",
    "print(\"Optimal Value Function:\", vi.V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "num_states = len(df[state_columns].value_counts())\n",
    "num_actions = len(df[action_columns].value_counts())\n",
    "Q = defaultdict(lambda: np.zeros((num_states, num_actions)))\n",
    "print(Q[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "class DynaQAgent:\n",
    "    def __init__(self, state_size, action_size, gamma=0.9, alpha=0.1, epsilon=0.1, planning_steps=10):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        self.epsilon = epsilon\n",
    "        self.planning_steps = planning_steps\n",
    "        self.q_table = np.zeros((state_size, action_size))\n",
    "        self.model = defaultdict(lambda: (None, 0))  #(next_state, reward)\n",
    "\n",
    "    def choose_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.action_size)\n",
    "        else:\n",
    "            return np.argmax(self.q_table[state])\n",
    "\n",
    "    def update(self, state, action, reward, next_state):\n",
    "        best_next_action = np.argmax(self.q_table[next_state])\n",
    "        td_target = reward + self.gamma * self.q_table[next_state, best_next_action]\n",
    "        self.q_table[state, action] += self.alpha * (td_target - self.q_table[state, action])\n",
    "\n",
    "        self.model[(state, action)] = (next_state, reward)\n",
    "\n",
    "        for _ in range(self.planning_steps):\n",
    "            sim_state, sim_action = random.choice(list(self.model.keys()))\n",
    "            sim_next_state, sim_reward = self.model[(sim_state, sim_action)]\n",
    "\n",
    "            sim_best_next_action = np.argmax(self.q_table[sim_next_state])\n",
    "            sim_td_target = sim_reward + self.gamma * self.q_table[sim_next_state, sim_best_next_action]\n",
    "            self.q_table[sim_state, sim_action] += self.alpha * (sim_td_target - self.q_table[sim_state, sim_action])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = {tuple(row): idx for idx, row in enumerate(df[state_columns].drop_duplicates().values)}\n",
    "action_dict = {tuple(row): idx for idx, row in enumerate(df[action_columns].drop_duplicates().values)}\n",
    "\n",
    "def get_state_index(row):\n",
    "    return state_dict[tuple(row[state_columns])]\n",
    "\n",
    "def get_action_index(row):\n",
    "    return action_dict[tuple(row[action_columns])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0): 0, (2, 0): 1, (1, 0): 2, (0, 1): 3, (0, 4): 4, (4, 0): 5, (2, 1): 6, (1, 1): 7, (4, 4): 8, (2, 4): 9, (4, 1): 10, (1, 4): 11}\n"
     ]
    }
   ],
   "source": [
    "print(action_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9109\n",
      "12\n",
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n",
      "60000\n",
      "61000\n",
      "62000\n",
      "63000\n",
      "64000\n",
      "65000\n",
      "66000\n",
      "67000\n",
      "68000\n",
      "69000\n",
      "70000\n",
      "71000\n",
      "72000\n",
      "73000\n",
      "74000\n",
      "75000\n",
      "76000\n",
      "77000\n",
      "78000\n",
      "79000\n",
      "80000\n",
      "81000\n",
      "82000\n",
      "83000\n",
      "84000\n",
      "85000\n",
      "86000\n",
      "87000\n",
      "88000\n",
      "89000\n",
      "90000\n",
      "91000\n",
      "92000\n",
      "93000\n",
      "94000\n",
      "95000\n",
      "96000\n",
      "97000\n",
      "98000\n",
      "99000\n",
      "100000\n",
      "101000\n",
      "102000\n",
      "103000\n",
      "104000\n",
      "105000\n",
      "106000\n",
      "107000\n",
      "108000\n",
      "109000\n",
      "110000\n",
      "111000\n",
      "112000\n",
      "113000\n",
      "114000\n",
      "115000\n",
      "116000\n",
      "117000\n",
      "118000\n",
      "119000\n",
      "120000\n",
      "121000\n",
      "122000\n",
      "123000\n",
      "124000\n",
      "125000\n",
      "126000\n",
      "127000\n",
      "128000\n",
      "129000\n",
      "130000\n",
      "131000\n",
      "132000\n",
      "133000\n",
      "134000\n",
      "135000\n",
      "136000\n",
      "137000\n",
      "138000\n",
      "139000\n",
      "140000\n",
      "141000\n",
      "142000\n",
      "143000\n",
      "144000\n",
      "145000\n",
      "146000\n",
      "147000\n",
      "148000\n",
      "149000\n",
      "150000\n"
     ]
    }
   ],
   "source": [
    "state_size = len(state_dict)\n",
    "action_size = len(action_dict)\n",
    "print(state_size)\n",
    "print(action_size)\n",
    "\n",
    "agent = DynaQAgent(state_size, action_size)\n",
    "\n",
    "i = 0\n",
    "for idx, row in train_df.iterrows():\n",
    "    state = get_state_index(row)\n",
    "    action = get_action_index(row)\n",
    "    if i % 1000 == 0:\n",
    "        print(i)\n",
    "    i += 1\n",
    "    next_state_row = train_df[(train_df['PatientID'] == row['PatientID']) & (train_df['Timepoints'] == row['Timepoints'] + 1)]\n",
    "    \n",
    "    if not next_state_row.empty:\n",
    "        next_state = get_state_index(next_state_row.iloc[0])\n",
    "        reward = calculate_reward(row['SOFA_score'], next_state_row.iloc[0]['SOFA_score'])\n",
    "        \n",
    "        agent.update(state, action, reward, next_state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cumulative reward (Dyna-Q) with SOFA score reward: -48.29999999999833\n"
     ]
    }
   ],
   "source": [
    "cumulative_reward = 0\n",
    "\n",
    "for idx, row in test_df.iterrows():\n",
    "    state = get_state_index(row)\n",
    "    action = agent.choose_action(state)\n",
    "\n",
    "    next_state_row = test_df[(test_df['PatientID'] == row['PatientID']) & (test_df['Timepoints'] == row['Timepoints'] + 1)]\n",
    "    \n",
    "    if not next_state_row.empty:\n",
    "        next_state = get_state_index(next_state_row.iloc[0])\n",
    "        reward = calculate_reward(row['SOFA_score'], next_state_row.iloc[0]['SOFA_score'])\n",
    "        \n",
    "        cumulative_reward += reward\n",
    "\n",
    "print(\"total cumulative reward (Dyna-Q) with SOFA score reward:\", cumulative_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total cumulative reward (clinician) with SOFA score reward: -70.92499999999883\n"
     ]
    }
   ],
   "source": [
    "def calculate_cumulative_reward(df):\n",
    "    cumulative_rewards = 0\n",
    "\n",
    "    for patient_id, patient_data in df.groupby('PatientID'):\n",
    "        patient_data = patient_data.sort_values('Timepoints')\n",
    "        total_reward = 0\n",
    "\n",
    "        for i in range(len(patient_data) - 1):\n",
    "            current_state = patient_data.iloc[i]\n",
    "            next_state = patient_data.iloc[i + 1]\n",
    "            reward = calculate_reward(current_state['SOFA_score'], next_state['SOFA_score'])\n",
    "            total_reward += reward\n",
    "\n",
    "        cumulative_rewards += total_reward\n",
    "\n",
    "    return cumulative_rewards\n",
    "\n",
    "cumulative_rewards = calculate_cumulative_reward(test_df)\n",
    "print(\"total cumulative reward (clinician) with SOFA score reward:\", cumulative_rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_action_cumulative_reward(states_df, transition_probs, initial_state, num_steps=100):\n",
    "    current_state = initial_state\n",
    "    cumulative_reward = 0\n",
    "\n",
    "    for step in range(num_steps):\n",
    "        current_index = states_df.index[states_df == current_state].tolist()[0]\n",
    "        next_state_index = np.random.choice(len(states_df), p=transition_probs[current_index, :, 0])\n",
    "        next_state = states_df.iloc[next_state_index]\n",
    "\n",
    "        reward = calculate_reward(current_state, next_state)\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "    return cumulative_reward\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
